{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud APIs for Computer Vision: Up and Running in 15 Minutes\n",
    "\n",
    "This code is part of [Chapter 8- Cloud APIs for Computer Vision: Up and Running in 15 Minutes ](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch08.html).\n",
    "\n",
    "## Test OCR from Cloud Providers\n",
    "\n",
    "This code sample details how one image can be uploaded to various cloud providers using the scripts in [`optical-character-recognition`](https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-8/experiment-scripts/optical-character-recognition) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the absolute path to the `data` directory (example: `data-may-2020`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"<PATH_TO_DATA_DIR>\"\n",
    "legible_images_path = data_path + \"/legible-images/\"\n",
    "os.path.exists(legible_images_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_from_image_id(image_id):\n",
    "    return \"COCO_train2014_000000\" + str(image_id) + \".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare how various cloud providers fare, let's view one example image and compare the results from the cloud providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_id = 229378\n",
    "\n",
    "image = io.imread(legible_images_path + filename_from_image_id(image_id))\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text in the image is pretty small and even with a normal human eyesight, it's pretty difficult to decipher all the words. Let's enlarge the image to really see all the text. We will use `mpld3` to enable zooming into the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as you hover over the plot, a toolbar appears in the lower left. This has tools to enable panning and zooming, and a button to reset the view once you've explored the plot.\n",
    "\n",
    "Press the magnification glass button to zoom. Then go to the spot where you want to zoom and drag click the mouse, dragging it to a new position. The X-axis will be zoomed in proportionate to the rightward movement and zoomed out proportionate to the leftward movement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's upload this image to the cloud providers and see how well they do. We will be using the Google, Microsoft, and Amazon cloud providers. \n",
    "\n",
    "Remember to do the following:\n",
    "\n",
    "- Register and generate an API key for each and replace it in the corresponding scripts for each cloud provider in the [`experiment-scripts`](https://github.com/PracticalDL/Practical-Deep-Learning-Book/tree/master/code/chapter-8/experiment-scripts) directory.\n",
    "- Replace the `<PATH_TO_LEGIBLE_DATA>` with the path to the `legible-data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_output_path = data_path + \"google-ocr-jsondump.json\"\n",
    "!python ../experiment-scripts/optical-character-recognition/google.py -i $legible_images_path/COCO_train2014_000000229378.jpg -o $google_output_path\n",
    "!python -m json.tool $google_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft\n",
    "\n",
    "Note: Unlike other providers, Microsoft requires that the images be hosted on a web URL which is accessible publicly. One option might be to use a cloud storage provider like Dropbox, OneDrive, etc. or host them on AWS S3 or Azure storage. We went with a different alternative, which is to run a web server on our own computer and point the script to our public IP address. If that's the route you choose to go, you can either install a LAMPP stack on your machine or simply create a temporary web service from the directory which contains the data as follows:\n",
    "\n",
    "`$ python3 -m http.server 80`\n",
    "\n",
    "Also, update the IP address url in `microsoft-phase-1.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_recognition_ids_path = data_path + \"/msft-recognition-ids.txt\"\n",
    "!python ../experiment-scripts/optical-character-recognition/microsoft-phase-1.py -i COCO_train2014_000000229378.jpg > $microsoft_recognition_ids_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "microsoft_output_path = data_path + \"/msft-ocr-jsondump.json\"\n",
    "!python ../experiment-scripts/optical-character-recognition/microsoft-phase-2.py -i $microsoft_recognition_ids_path -o $microsoft_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m json.tool $microsoft_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "amazon_output_path = data_path + \"/amazon-ocr-jsondump.json\"\n",
    "!python ../experiment-scripts/optical-character-recognition/amazon.py -i $legible_images_path/COCO_train2014_000000229378.jpg -o $amazon_output_path\n",
    "!python -m json.tool $amazon_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Different cloud providers are able to pick up different words in the image. But what about running against all images, after all that is how we will be able to generate a useful benchmark. Let's look at that next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
