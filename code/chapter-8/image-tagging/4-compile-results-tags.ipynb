{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud APIs for Computer Vision: Up and Running in 15 Minutes\n",
    "\n",
    "This code is part of [Chapter 8- Cloud APIs for Computer Vision: Up and Running in 15 Minutes ](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch08.html).\n",
    "\n",
    "## Compile Results for Image Tagging\n",
    "\n",
    "In this file we will compile the results using the ground truth and the collected data for all the test images. You will need to edit the following: \n",
    "\n",
    "1. Please edit `data_path` with the path to the test images that have been used for the experiments. \n",
    "2. If you used different filenames for the prediction filenames, please edit the filenames accordingly.\n",
    "3. Please download Gensim, which we will be using for comparing word similarity between ground truth with predicted class. Unzip and place the `GoogleNews-vectors-negative300.bin` within `data_path`. Download at: https://github.com/mmihaltz/word2vec-GoogleNews-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the ground truth JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/deepvision/production/code/chapter-8/image-tagging/data-may-2020\"\n",
    "validation_images_path = data_path + \"/val2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"/final-ground-truth-tags.json\") as json_file:\n",
    "    ground_truth = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get image name from image id and converse.\n",
    "def get_id_from_name(name):\n",
    "    return int(name.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "\n",
    "def get_name_from_id(image_id):\n",
    "    filename = validation_images_path + \\\n",
    "        \"/000000\" + str(image_id) + \".jpg\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class ids to their string equivalent\n",
    "with open(data_path + '/class-id-to-name.json') as f:\n",
    "    class_id_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_class_id_to_string(l):\n",
    "    result = []\n",
    "    for class_id in l:\n",
    "        result.append(class_id_to_name[str(class_id)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(l):\n",
    "    l1 = []\n",
    "    for each in l:\n",
    "        if len(each) >= 2:\n",
    "            l1.append(each.lower())\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_prediction(l):\n",
    "    return list([item[0] for item in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download Gensim, which we will be using for comparing word similarity between ground truth with predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(data_path + \n",
    "    '/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gensim(word, pred):\n",
    "    # get similarity between word and all predicted words in returned predictions\n",
    "    similarity = 0\n",
    "    for each_pred in pred:\n",
    "        # check if returned prediction exists in the Word2Vec model\n",
    "        if each_pred not in model:\n",
    "            continue\n",
    "        current_similarity = model.similarity(word, each_pred)\n",
    "        #print(\"Word=\\t\", word, \"\\tPred=\\t\", each_pred, \"\\tSim=\\t\", current_similarity)\n",
    "        if current_similarity > similarity:\n",
    "            similarity = current_similarity\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Parsing\n",
    "\n",
    "Each cloud provider sends the results in slightly different formats and we need to parse each of them correctly. So, we will develop a parsing function unique to each cloud provider.\n",
    "\n",
    "#### Microsoft Specific Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def microsoft_name(image_id):\n",
    "    return \"000000\" + str(image_id) + \".jpg\"\n",
    "\n",
    "def parse_microsoft_inner(word):\n",
    "    b = word.replace(\"_\", \" \")\n",
    "    c = b.lower().strip().split()\n",
    "    return c\n",
    "\n",
    "def parse_microsoft_response_v1(l):\n",
    "    result = []\n",
    "    b = \"\"\n",
    "    for each in l[\"categories\"]:\n",
    "        a = each[\"name\"]\n",
    "        result.extend(parse_microsoft_inner(a))\n",
    "    for each in l[\"tags\"]:\n",
    "        a = each[\"name\"]\n",
    "        result.extend(parse_microsoft_inner(a))\n",
    "        if \"hint\" in each:\n",
    "            a = each[\"hint\"]\n",
    "            result.extend(parse_microsoft_inner(a))\n",
    "    return list(set(result))\n",
    "\n",
    "def parse_microsoft_response(l):\n",
    "    result = []\n",
    "    b = \"\"\n",
    "    for each in l:\n",
    "        result.extend(parse_microsoft_inner(each[0]))\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Specific Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_amazon_response(l):\n",
    "    result = []\n",
    "    for each in l:\n",
    "        result.append(each.lower())\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google specific parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_google_response(l):\n",
    "    l1 = []\n",
    "    for each in l:\n",
    "        l1.append(each[0].lower())\n",
    "        if len(each[0].split()) > 1:\n",
    "            l1.extend(each[0].split())\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `threshold` defines how much similar do two words (ground truth and predicted category name) need to be according to Word2Vec for the prediction to be a correct prediction. You can play around with the `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .3\n",
    "\n",
    "def calculate_score(ground_truth, predictions, arg):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    avg_ground_truth_length = 0\n",
    "    avg_amazon_length = 0\n",
    "    avg_microsoft_length = 0\n",
    "    avg_google_length = 0\n",
    "    for each in ground_truth.keys():\n",
    "        pred = []\n",
    "        gt = list(set(convert_class_id_to_string(ground_truth[each])))\n",
    "        if gt == None or len(gt) < 1:\n",
    "            continue\n",
    "        total += len(gt)\n",
    "        avg_ground_truth_length += len(gt)\n",
    "        if arg == \"google\" and get_name_from_id(each) in predictions:\n",
    "            pred = predictions[get_name_from_id(each)]\n",
    "            if pred == None or len(pred) <= 0:\n",
    "                continue\n",
    "            pred = parse_google_response(predictions[get_name_from_id(each)])\n",
    "            avg_google_length += len(pred)\n",
    "        elif arg == \"microsoft\" and microsoft_name(each) in predictions:\n",
    "            pred = predictions[microsoft_name(each)]\n",
    "            if pred == None or len(pred) <= 0:\n",
    "                continue\n",
    "            pred = parse_microsoft_response(predictions[microsoft_name(each)])\n",
    "            avg_microsoft_length += len(pred)\n",
    "        elif arg == \"amazon\" and get_name_from_id(each) in predictions:\n",
    "            pred = predictions[get_name_from_id(each)]\n",
    "            if pred == None or len(pred) <= 0:\n",
    "                continue\n",
    "            pred = parse_amazon_response(predictions[get_name_from_id(each)])\n",
    "            avg_amazon_length += len(pred)\n",
    "        match = 0\n",
    "        match_word = []\n",
    "        for each_word in gt:\n",
    "            # Check if ground truth exists \"as is\" in the entire list of predictions\n",
    "            if each_word in pred:\n",
    "                correct += 1\n",
    "                match += 1\n",
    "                match_word.append(each_word)\n",
    "            # Also, ensure that ground truth exists in the Word2Vec model\n",
    "            elif each_word not in model:\n",
    "                continue\n",
    "            # Otherwise, check for similarity between the ground truth and the predictions\n",
    "            elif check_gensim(each_word, pred) >= threshold:\n",
    "                correct += 1\n",
    "                match += 1\n",
    "                match_word.append(each_word)\n",
    "    if arg == \"google\":\n",
    "        print(\"Google's Stats\\nTotal number of tags returned = \", avg_google_length,\n",
    "              \"\\nAverage number of tags returned per image = \",\n",
    "              avg_google_length * 1.0 / len(ground_truth.keys()))\n",
    "    elif arg == \"amazon\":\n",
    "        print(\"Amazon's Stats\\nTotal number of tags returned = \", avg_amazon_length,\n",
    "              \"\\nAverage number of tags returned per image = \",\n",
    "              avg_amazon_length * 1.0 / len(ground_truth.keys()))\n",
    "    elif arg == \"microsoft\":\n",
    "        print(\"Microsoft's Stats\\nTotal number of tags returned = \",\n",
    "              avg_microsoft_length, \"\\nAverage number of tags returned per image = \",\n",
    "              avg_microsoft_length * 1.0 / len(ground_truth.keys()))\n",
    "    print(\"\\nGround Truth Stats\\nTotal number of Ground Truth tags = \", total,\n",
    "          \"\\nTotal number of correct tags predicted = \", correct)\n",
    "    print(\"\\nScore = \", float(correct) / float(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to load the predictions that we obtained by using APIs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google\n",
    "with open(data_path + '/google-tags.json') as f:\n",
    "    google = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Google Score\n",
    "calculate_score(ground_truth, google, \"google\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Microsoft's API for object classification has two versions. The results from both the APIs are different. \n",
    "\n",
    "If you want to check out Microsoft's outdated (v1) API then use the `microsoft_tags.json` file. We will be using the latest version (i.e., `microsoft_tags_DESCRIPTION.json`) for our November 2019 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft\n",
    "with open(data_path + '/microsoft-tags.json') as f:\n",
    "    microsoft = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Microsoft score\n",
    "calculate_score(ground_truth, microsoft, \"microsoft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon\n",
    "with open(data_path + '/amazon-tags.json') as f:\n",
    "    amazon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Amazon score\n",
    "calculate_score(ground_truth, amazon, \"amazon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
