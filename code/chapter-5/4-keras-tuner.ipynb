{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rz6kHwluVbGG"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/4-keras-tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/4-keras-tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "    IS_COLAB_ENV = True\n",
    "except:\n",
    "    IS_COLAB_ENV = False\n",
    "IS_COLAB_ENV"
   ],
   "metadata": {
    "id": "mzBGiFfvVk_P",
    "outputId": "0140edc8-99a1-41eb-b41f-937da68cf5b3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSg8GnCYVbGH"
   },
   "source": [
    "# Keras Tuner\n",
    "\n",
    "With so many potential combinations of hyperparameters to tune, coming up with the best model can be a tedious process. Often two or more parameters might have correlated effects on the overall speed of convergence as well as validation accuracy, so tuning one at a time might not lead to the best model.  And if curiosity gets the best of us, we might want to experimentation on all the hyperparameters together!  \n",
    "\n",
    "Keras Tuner comes to automate this hyperparameter search. We define a search algorithm, the potential values that each parameter can take (e.g. discrete values or a range), our target object to maximize (e.g. validation accuracy) and sit back to see the program start training. Keras Tuner will conduct multiple experiments changing the parameters on our behalf, storing the metadata of the best model. The following code example adapted from Keras Tuner documentation showcases searching through the different model architectures (varying in the number of layers between 2 and 10) as well as varying the learning rate (between 0.1 and 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8RT-_F7JVbGI",
    "outputId": "79950b1c-d842-4fde-8f64-167cb601a50c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/176.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "NqZ6vLJRV79N"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UgkirlmxVbGI",
    "outputId": "5fe701e9-71d8-4006-91e3-2514913420cc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-52e813ccbcd6>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.engine.hypermodel import HyperModel\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.engine.hypermodel import HyperModel\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y5p5xqIhVbGI",
    "outputId": "da9943bd-a6e5-467d-a059-1627b753c0df",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "(x, y), (val_x, val_y) = keras.datasets.mnist.load_data()\n",
    "x = x.astype(\"float32\") / 255.0\n",
    "val_x = val_x.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TXbOoVPSVbGI",
    "outputId": "157edd6e-0bff-448f-b7aa-9306d37a2a75",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Defining hyper parameters\n",
    "hp = HyperParameters()\n",
    "hp.Choice(\"learning_rate\", [0.1, 0.001])\n",
    "hp.Int(\"num_layers\", 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wKQ1Qv3ZVbGI"
   },
   "outputs": [],
   "source": [
    "# Defining model with expandable number of layers\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    for i in range(hp.get(\"num_layers\")):\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get(\"learning_rate\")),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Gkb3p-ABVbGJ"
   },
   "outputs": [],
   "source": [
    "hypermodel = RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=20,  # Number of combinations allowed\n",
    "    hyperparameters=hp,\n",
    "    allow_new_entries=False,\n",
    "    objective=\"val_accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "N79d4gAbVbGJ",
    "outputId": "3bf41e28-6053-401b-da59-37bf168e8054",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 18 Complete [00h 00m 37s]\n",
      "val_accuracy: 0.11349999904632568\n",
      "\n",
      "Best val_accuracy So Far: 0.9672999978065491\n",
      "Total elapsed time: 00h 11m 18s\n"
     ]
    }
   ],
   "source": [
    "hypermodel.search(x=x, y=y, epochs=5, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vIbENAVjVbGJ",
    "outputId": "675a632b-c906-43f6-ecfb-4b2503a5a8aa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 3\n",
      "Score: 0.9672999978065491\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 4\n",
      "Score: 0.9663000106811523\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 2\n",
      "Score: 0.964900016784668\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 8\n",
      "Score: 0.9639999866485596\n",
      "\n",
      "Trial 11 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 5\n",
      "Score: 0.9639000296592712\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 10\n",
      "Score: 0.9635000228881836\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 6\n",
      "Score: 0.9624999761581421\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 7\n",
      "Score: 0.9623000025749207\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "num_layers: 9\n",
      "Score: 0.9560999870300293\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.1\n",
      "num_layers: 2\n",
      "Score: 0.2102999985218048\n"
     ]
    }
   ],
   "source": [
    "# Show summary of overall best model\n",
    "hypermodel.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}