{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/3-tf-explain.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/PracticalDL/Practical-Deep-Learning-Book/blob/master/code/chapter-5/3-tf-explain.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf-explain`\n",
    "\n",
    "tf-explain (by Raphael Meudec) helps understand the results and inner workings of a neural network with the help of visualizations, removing the veil on bias in our datasets. Few different visualization approaches are available with tf.explain.\n",
    "\n",
    "- Grad CAM: The Gradient-weighted Class Activation Mapping visualizes how parts of the image affect the neural network's output by looking into the activation maps. A heatmap is generated based on the gradients of the object id from the last convolutional layer. Grad CAM is largely a broad-spectrum heatmap generator as it is robust to noise and can be used on an array of CNN models. \n",
    "- Activations: Visualize the activations for the convolutional layers. \n",
    "- Occlusion Sensitivity: Occludes a part of the image (using a small square patch placed randomly) to figure out how robust the network is. If the prediction is still correct, on average, the network is robust. The area in the image that is the warmest (i.e. red) has the most effect on the prediction when occluded.\n",
    "\n",
    "In this notebook we will produce different visualizations on the sample images.\n",
    "\n",
    "Note: After executing the first cell you may have to `RESTART RUNTIME` if you are running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "2v99NSOQqomy",
    "outputId": "0d72633f-9455-4f94-91ec-467c0e5c3219"
   },
   "outputs": [],
   "source": [
    "# Perform all installations\n",
    "!pip install tensorflow-gpu==2.0.0\n",
    "!pip install tf-explain==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Get TensorBoard to run\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add multiple types of callbacks while training or use its core API to generate TensorFlow events that can later be loaded into TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hb5g7QTWREM-"
   },
   "outputs": [],
   "source": [
    "from tf_explain.core.grad_cam import GradCAM\n",
    "from tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n",
    "from tf_explain.core.activations import ExtractActivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrELWre7VuaN"
   },
   "source": [
    "For inference, all we need to do is pass an image, its ImageNet object ID along with a model into tf-explainâ€™s functions. The object id is needed as tf.explain needs to figure out what is activated for that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB_ENV = True\n",
    "except:\n",
    "  IS_COLAB_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_image(filename):\n",
    "    import requests\n",
    "    url = f'https://raw.githubusercontent.com/PracticalDL/Practical-Deep-Learning-Book/master/sample-images/{filename}'\n",
    "    open(filename, 'wb').write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB_ENV:\n",
    "    IMAGE_PATHS = ['dog.jpg', 'cat.jpg']\n",
    "    for each_filename in IMAGE_PATHS:\n",
    "        download_sample_image(each_filename)\n",
    "else:\n",
    "    IMAGE_PATHS = [ '../../sample-images/dog.jpg', '../../sample-images/cat.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "94_DTMmIqjOQ",
    "outputId": "81913371-213b-478d-c0dc-f36199184bc5"
   },
   "outputs": [],
   "source": [
    "indices = [263, 281]\n",
    "\n",
    "layers_name = ['activation_6']\n",
    "\n",
    "for i in range(len(IMAGE_PATHS)):\n",
    "    each_path = IMAGE_PATHS[i]\n",
    "    index = indices[i]\n",
    "\n",
    "    img = tf.keras.preprocessing.image.load_img(each_path,\n",
    "                                                target_size=(224, 224))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    data = ([img], None)\n",
    "    # Define name with which to save the result as\n",
    "    name = each_path.split(\"/\")[-1].split(\".jpg\")[0]\n",
    "\n",
    "    #Save the Grad Cam visualization\n",
    "    explainer = GradCAM()\n",
    "    model = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                              include_top=True)\n",
    "    grid = explainer.explain(data, model, 'block5_conv3', index)\n",
    "    explainer.save(grid, '.', name + 'grad_cam.png')\n",
    "\n",
    "    # Save the Occlusion Sensitivity visualization\n",
    "    explainer = OcclusionSensitivity()\n",
    "    model = tf.keras.applications.resnet50.ResNet50(weights='imagenet',\n",
    "                                                    include_top=True)\n",
    "    # Compute Occlusion Sensitivity for patch_size 20\n",
    "    grid = explainer.explain(data, model, index, 20)\n",
    "    explainer.save(grid, '.', name + 'occlusion_sensitivity_20.png')\n",
    "    # Compute Occlusion Sensitivity for patch_size 10\n",
    "    grid = explainer.explain(data, model, index, 10)\n",
    "    explainer.save(grid, '.', name + 'occlusion_sensitivity_10.png')\n",
    "\n",
    "    # Save the Activations visualizations\n",
    "    data = (np.array([img]), None)\n",
    "    explainer = ExtractActivations()\n",
    "    model = tf.keras.applications.mobilenet.MobileNet(weights='imagenet',\n",
    "                                                      include_top=True)\n",
    "    grid = explainer.explain(data, model, ['conv1'])\n",
    "    explainer.save(grid, '.', name + 'activations.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gradcam-tf-explain-catdog.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
