{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-4/1-feature-extraction.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-4/1-feature-extraction.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "This code is part of [Chapter 4 - Building a Reverse Image Search Engine: Understanding Embeddings](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch04.html).\n",
    "\n",
    "Note: In order to run this notebook on Google Colab you need to [follow these instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#scrollTo=WzIRIt9d2huC) so that the local data such as the images are available in your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "This notebook is the first among six of the follow along Jupyter Notebook for Chapter 4. We will extract features from pretrained models like VGG-16, VGG-19, ResNet-50, InceptionV3 and MobileNet and benchmark them using the Caltech101 dataset.\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "In the `data` directory of the repo, download the Caltech101 dataset (or try it on your dataset). Please note (as of 01 September 2020) the Caltech 101 dataset has moved locations and now has to be downloaded through Google Drive using `gdown`.\n",
    "\n",
    "```\n",
    "$ gdown https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp --output caltech101.tar.gz\n",
    "\n",
    "$ tar -xvzf caltech101.tar.gz\n",
    "\n",
    "$ mv 101_ObjectCategories datasets/caltech101\n",
    "```\n",
    "Note that there is a 102nd category called ‘BACKGROUND_Google’ consisting of random images not contained in the first 101 categories, which needs to be deleted before we start experimenting. \n",
    "\n",
    "```\n",
    "$ rm -rf datasets/caltech101/BACKGROUND_Google\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p ../../datasets\n",
    "!mkdir data\n",
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp --output ../../datasets/caltech101.tar.gz\n",
    "!tar -xvzf ../../datasets/caltech101.tar.gz --directory ../../datasets\n",
    "!mv ../../datasets/101_ObjectCategories ../../datasets/caltech101\n",
    "!rm -rf ../../datasets/caltech101/BACKGROUND_Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a helper function that allows us to choose any pretrained model with all the necessary details for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_picker(name):\n",
    "    if (name == 'vgg16'):\n",
    "        model = VGG16(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(224, 224, 3),\n",
    "                      pooling='max')\n",
    "    elif (name == 'vgg19'):\n",
    "        model = VGG19(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(224, 224, 3),\n",
    "                      pooling='max')\n",
    "    elif (name == 'mobilenet'):\n",
    "        model = MobileNet(weights='imagenet',\n",
    "                          include_top=False,\n",
    "                          input_shape=(224, 224, 3),\n",
    "                          pooling='max',\n",
    "                          depth_multiplier=1,\n",
    "                          alpha=1)\n",
    "    elif (name == 'inception'):\n",
    "        model = InceptionV3(weights='imagenet',\n",
    "                            include_top=False,\n",
    "                            input_shape=(224, 224, 3),\n",
    "                            pooling='max')\n",
    "    elif (name == 'resnet'):\n",
    "        model = ResNet50(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(224, 224, 3),\n",
    "                        pooling='max')\n",
    "    elif (name == 'xception'):\n",
    "        model = Xception(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(224, 224, 3),\n",
    "                         pooling='max')\n",
    "    else:\n",
    "        print(\"Specified model not available\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put our function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_architecture = 'resnet'\n",
    "model = model_picker(model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to extract image features given an image and a model. We developed a similar function in Chapter-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(img_path, model):\n",
    "    input_shape = (224, 224, 3)\n",
    "    img = image.load_img(img_path,\n",
    "                         target_size=(input_shape[0], input_shape[1]))\n",
    "    img_array = image.img_to_array(img)\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    features = model.predict(preprocessed_img)\n",
    "    flattened_features = features.flatten()\n",
    "    normalized_features = flattened_features / norm(flattened_features)\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the feature length the model generates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = extract_features('../../sample-images/cat.jpg', model)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how much time it takes to extract features of one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit features = extract_features('../../sample-images/cat.jpg', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time taken to extract features is dependent on a few factors such as image size, computing power etc. A better benchmark would be running the network over an entire dataset. A simple change to the existing code will allow this.\n",
    "\n",
    "Let's make a handy function to recursively get all the image files under a root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
    "\n",
    "def get_file_list(root_dir):\n",
    "    file_list = []\n",
    "    for root, directories, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if any(ext in filename for ext in extensions):\n",
    "                file_list.append(os.path.join(root, filename))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the extraction over the entire dataset and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the your datasets\n",
    "root_dir = '../../datasets/caltech101'\n",
    "filenames = sorted(get_file_list(root_dir))\n",
    "\n",
    "feature_list = []\n",
    "for i in tqdm_notebook(range(len(filenames))):\n",
    "    feature_list.append(extract_features(filenames[i], model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same with the Keras Image Generator functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "generator = datagen.flow_from_directory(root_dir,\n",
    "                                        target_size=(224, 224),\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode=None,\n",
    "                                        shuffle=False)\n",
    "\n",
    "num_images = len(generator.filenames)\n",
    "num_epochs = int(math.ceil(num_images / batch_size))\n",
    "\n",
    "start_time = time.time()\n",
    "feature_list = []\n",
    "feature_list = model.predict_generator(generator, num_epochs)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, features in enumerate(feature_list):\n",
    "    feature_list[i] = features / norm(features)\n",
    "\n",
    "feature_list = feature_list.reshape(num_images, -1)\n",
    "\n",
    "print(\"Num images   = \", len(generator.classes))\n",
    "print(\"Shape of feature_list = \", feature_list.shape)\n",
    "print(\"Time taken in sec = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Utilization's effect on time taken by varying batch size \n",
    "\n",
    "\n",
    "GPUs are optimized to parallelize the feature generation process and hence will give better results when multiple images are passed instead of just one image.\n",
    "The opportunity to improve can be seen based on GPU Utilization. Low GPU Utilization indicates an opportunity to further improve the througput.\n",
    "\n",
    "\n",
    "GPU Utilization can be seen using the nvidia-smi command. To update it every half a second\n",
    "\n",
    "    watch -n .5 nvidia-smi\n",
    "    \n",
    "To pool the GPU utilization every second and dump into a file\n",
    "\n",
    "    nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits -f gpu_utilization.csv -l 1\n",
    "    \n",
    "To calculate median GPU Utilization from the file generated\n",
    "\n",
    "    sort -n gpu_utilization.csv | datamash median 1\n",
    "\n",
    "|Model |Time second (sec) | batch_size | % GPU Utilization | Implementation|\n",
    "|-|-|-|\n",
    "|Resnet50 | 124  | 1  | 52 | extract_features    |\n",
    "|Resnet50 | 98   | 1  | 72 | ImageDataGenerator |\n",
    "|Resnet50 | 57   | 2  | 81 | ImageDataGenerator |\n",
    "|Resnet50 | 40   | 4  | 88 | ImageDataGenerator |\n",
    "|Resnet50 | 34   | 8  | 94 | ImageDataGenerator |\n",
    "|Resnet50 | 29   | 16 | 97 | ImageDataGenerator |\n",
    "|Resnet50 | 28   | 32 | 97 | ImageDataGenerator |\n",
    "|Resnet50 | 28   | 64 | 98 | ImageDataGenerator |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some benchmarks on different model architectures to see relative speeds\n",
    "\n",
    "Keeping batch size of 64, benchmarking the different models\n",
    "\n",
    "|Model |items/second |\n",
    "|-|-|-|\n",
    "| VGG19     | 31.06 |\n",
    "| VGG16     | 28.16 | \n",
    "| Resnet50  | 28.48 | \n",
    "| Inception | 20.07 |\n",
    "| Mobilenet | 13.45 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the features as intermediate files to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [root_dir + '/' + s for s in generator.filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(generator.classes, open('./data/class_ids-caltech101.pickle',\n",
    "                                    'wb'))\n",
    "pickle.dump(filenames, open('./data/filenames-caltech101.pickle', 'wb'))\n",
    "pickle.dump(\n",
    "    feature_list,\n",
    "    open('./data/features-caltech101-' + model_architecture + '.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a finetuned model as well and save the features for that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SAMPLES = 8677\n",
    "NUM_CLASSES = 101\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(root_dir,\n",
    "                                                    target_size=(IMG_WIDTH,\n",
    "                                                                 IMG_HEIGHT),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=12345,\n",
    "                                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_maker():\n",
    "    base_model = ResNet50(include_top=False,\n",
    "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Dense(64, activation='relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
    "    return Model(inputs=input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_finetuned = model_maker()\n",
    "model_finetuned.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
    "              metrics=['acc'])\n",
    "model_finetuned.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / batch_size),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_finetuned.save('./data/model-finetuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "feature_list_finetuned = []\n",
    "feature_list_finetuned = model_finetuned.predict_generator(generator, num_epochs)\n",
    "end_time = time.time()\n",
    "\n",
    "for i, features_finetuned in enumerate(feature_list_finetuned):\n",
    "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\n",
    "\n",
    "feature_list = feature_list_finetuned.reshape(num_images, -1)\n",
    "\n",
    "print(\"Num images   = \", len(generator.classes))\n",
    "print(\"Shape of feature_list = \", feature_list.shape)\n",
    "print(\"Time taken in sec = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    feature_list,\n",
    "    open('./data/features-caltech101-resnet-finetuned.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
